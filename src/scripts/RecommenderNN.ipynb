{"cells":[{"cell_type":"markdown","metadata":{"id":"SE4ueQmLsxaB"},"source":["# Movie Recommender based on NN"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":5452,"status":"ok","timestamp":1720537792017,"user":{"displayName":"Maximilian Summerer","userId":"08060633434699036465"},"user_tz":-120},"id":"J2R9-82Vssgp","colab":{"base_uri":"https://localhost:8080/"},"outputId":"bb24a577-7a85-4a2e-ea82-e49a0683d392"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from keras.models import Model\n","from keras.layers import Input, Embedding, Flatten, Dense, concatenate\n","from keras.optimizers import Adam\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n","from keras.regularizers import l2\n","from keras.callbacks import Callback, ModelCheckpoint\n","from sklearn.metrics import mean_absolute_error\n","from sklearn.preprocessing import MinMaxScaler\n","from keras.models import load_model\n","import pickle\n","import matplotlib.pyplot as plt\n","\n","# Connect with Google Drive\n","import os\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","os.chdir('/content/drive/MyDrive/RecSys/')"]},{"cell_type":"code","source":["def save_history(history, filename='training_history.pkl'):\n","    with open(filename, 'wb') as file:\n","        pickle.dump(history.history, file)\n","\n","def load_history(filename='training_history.pkl'):\n","    with open(filename, 'rb') as file:\n","        return pickle.load(file)\n","\n","def save_movie_embeddings(movie_embeddings, filename='movie_embeddings.pkl'):\n","    with open(filename, 'wb') as file:\n","        pickle.dump(movie_embeddings, file)\n","\n","def load_movie_embeddings(filename='movie_embeddings.pkl'):\n","    with open(filename, 'rb') as file:\n","        return pickle.load(file)"],"metadata":{"id":"3iljpIHEHulu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def plot_history(history):\n","    # Plot training & validation loss values\n","    plt.plot(history.history['loss'])\n","    plt.plot(history.history['val_loss'])\n","    plt.title('Model loss')\n","    plt.ylabel('Loss')\n","    plt.xlabel('Epoch')\n","    plt.legend(['Train', 'Validation'], loc='upper left')\n","    plt.show()"],"metadata":{"id":"0G4eoPbCHzPO"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9S5JbYGntf2u"},"outputs":[],"source":["# Load the dataset\n","def load_data():\n","    # Define the column names explicitly\n","    columns_ratings = ['user_id', 'movie_id', 'rating', 'timestamp']\n","    # Define the column names explicitly\n","    columns_movies = ['movie_id', 'title', 'genres']\n","    # Read the CSV file, specifying the column names and skipping the first row\n","    ratings = pd.read_csv('ml-latest/ratings.csv', sep=',', names=columns_ratings, skiprows=1)\n","    # ratings = pd.read_csv('ml-latest/ratings.csv', sep='\\t', names=['user_id', 'movie_id', 'rating', 'timestamp'])\n","    movies = pd.read_csv('ml-latest/movies.csv',  sep=',', names=columns_movies, skiprows=1)\n","    # Keep only the movie_id and title columns\n","    movies = movies[['movie_id', 'title']]\n","    return ratings, movies\n","\n","# Standardize the ratings\n","def standardize_ratings(train_ratings, val_ratings):\n","    scaler = MinMaxScaler()\n","\n","    train_ratings['rating'] = scaler.fit_transform(train_ratings[['rating']])\n","    val_ratings['rating'] = scaler.transform(val_ratings[['rating']])\n","\n","    return train_ratings, val_ratings, scaler\n","\n","# Unstandardize the ratings\n","def unstandardize_ratings(ratings, scaler):\n","  ratings = np.array(ratings).reshape(-1, 1)\n","  return scaler.inverse_transform(ratings).flatten()\n","\n","# Preprocess the data\n","def preprocess_data(ratings, movies):\n","    user_ids = ratings['user_id'].unique().tolist()\n","    user_id_to_index = {x: i for i, x in enumerate(user_ids)}\n","    movie_ids = ratings['movie_id'].unique().tolist()\n","    movie_id_to_index = {x: i for i, x in enumerate(movie_ids)}\n","\n","    ratings['user_id'] = ratings['user_id'].map(user_id_to_index)\n","    ratings['movie_id'] = ratings['movie_id'].map(movie_id_to_index)\n","\n","    num_users = len(user_ids)\n","    num_movies = len(movie_ids)\n","\n","    return ratings, num_users, num_movies, user_id_to_index, movie_id_to_index\n","\n","# Build the neural network model\n","def build_model(num_users, num_movies, embedding_size=50):\n","    user_input = Input(shape=(1,), name='user_input')\n","    user_embedding = Embedding(input_dim=num_users, output_dim=embedding_size, name='user_embedding')(user_input)\n","    user_vec = Flatten(name='user_flatten')(user_embedding)\n","\n","    movie_input = Input(shape=(1,), name='movie_input')\n","    movie_embedding = Embedding(input_dim=num_movies, output_dim=embedding_size, name='movie_embedding')(movie_input)\n","    movie_vec = Flatten(name='movie_flatten')(movie_embedding)\n","\n","    concat = concatenate([user_vec, movie_vec], axis=-1, name='concat')\n","    dense = Dense(128, activation='relu', name='dense')(concat)\n","    output = Dense(1, activation='linear', name='output')(dense)\n","\n","    model = Model(inputs=[user_input, movie_input], outputs=output)\n","    model.compile(optimizer=Adam(learning_rate=0.00015), loss='mse')\n","\n","    return model\n","\n","# Train the model\n","def train_model(model, ratings, epochs=10, batch_size=256, checkpoint_filepath='best_model_try2.h5'):\n","\n","    # Manually split the data into training and validation sets\n","    train_ratings, val_ratings = train_test_split(ratings, test_size=0.2, random_state=42)\n","    # Let's standardize the ratings\n","    train_ratings, val_ratings, scaler = standardize_ratings(train_ratings, val_ratings)\n","\n","    user_input_train = train_ratings['user_id'].values\n","    movie_input_train = train_ratings['movie_id'].values\n","    targets_train = train_ratings['rating'].values\n","\n","    user_input_val = val_ratings['user_id'].values\n","    movie_input_val = val_ratings['movie_id'].values\n","    targets_val = val_ratings['rating'].values\n","\n","    # Learning rate scheduler\n","    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, min_lr=1e-6)\n","\n","    # Early stopping to prevent overfitting\n","    early_stopping = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n","\n","    # Model checkpoint to save the model when validation loss improves\n","    model_checkpoint = ModelCheckpoint(filepath=checkpoint_filepath, monitor='val_loss', save_best_only=True, verbose=1)\n","\n","    # Train the model with early stopping, learning rate reduction on plateau, and custom validation callback\n","    history = model.fit([user_input_train, movie_input_train], targets_train,\n","                        epochs=epochs, batch_size=batch_size,\n","                        validation_data=([user_input_val, movie_input_val], targets_val),\n","                        callbacks=[early_stopping, reduce_lr, model_checkpoint],\n","                        validation_freq= 1)\n","\n","    return history\n","\n","# Generate movie embeddings\n","def get_movie_embeddings(model, num_movies):\n","    movie_layer = model.get_layer('movie_embedding')\n","    movie_weights = movie_layer.get_weights()[0]\n","    return movie_weights\n","\n","# Find similar movies using learned embeddings\n","def find_similar_movies_nn(movie_ids, movie_embeddings, movie_id_to_index, movie_index_to_id, k=10):\n","    movie_indices = [movie_id_to_index[movie_id] for movie_id in movie_ids]\n","    movie_vecs = movie_embeddings[movie_indices]\n","\n","    avg_movie_vec = np.mean(movie_vecs, axis=0)\n","\n","    similarities = np.dot(movie_embeddings, avg_movie_vec)\n","    similar_indices = np.argsort(similarities)[::-1]\n","\n","    similar_movie_ids = [movie_index_to_id[idx] for idx in similar_indices if idx not in movie_indices][:k]\n","    return similar_movie_ids"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":838},"id":"TvziMJY5t5i6","executionInfo":{"status":"error","timestamp":1720543703102,"user_tz":-120,"elapsed":222508,"user":{"displayName":"Maximilian Summerer","userId":"08060633434699036465"}},"outputId":"56086a0a-9f04-4aa0-f32e-7f586c480d05"},"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch 1/10\n","105726/105726 [==============================] - ETA: 0s - loss: 0.0376\n","Epoch 1: val_loss improved from inf to 0.03489, saving model to best_model_try2.h5\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["105726/105726 [==============================] - 764s 7ms/step - loss: 0.0376 - val_loss: 0.0349 - lr: 1.5000e-04\n","Epoch 2/10\n","105726/105726 [==============================] - ETA: 0s - loss: 0.0333\n","Epoch 2: val_loss improved from 0.03489 to 0.03295, saving model to best_model_try2.h5\n","105726/105726 [==============================] - 731s 7ms/step - loss: 0.0333 - val_loss: 0.0330 - lr: 1.5000e-04\n","Epoch 3/10\n","105726/105726 [==============================] - ETA: 0s - loss: 0.0309\n","Epoch 3: val_loss improved from 0.03295 to 0.03198, saving model to best_model_try2.h5\n","105726/105726 [==============================] - 733s 7ms/step - loss: 0.0309 - val_loss: 0.0320 - lr: 1.5000e-04\n","Epoch 4/10\n","105725/105726 [============================>.] - ETA: 0s - loss: 0.0292\n","Epoch 4: val_loss improved from 0.03198 to 0.03137, saving model to best_model_try2.h5\n","105726/105726 [==============================] - 731s 7ms/step - loss: 0.0292 - val_loss: 0.0314 - lr: 1.5000e-04\n","Epoch 5/10\n","105726/105726 [==============================] - ETA: 0s - loss: 0.0276\n","Epoch 5: val_loss improved from 0.03137 to 0.03110, saving model to best_model_try2.h5\n","105726/105726 [==============================] - 730s 7ms/step - loss: 0.0276 - val_loss: 0.0311 - lr: 1.5000e-04\n","Epoch 6/10\n","105721/105726 [============================>.] - ETA: 0s - loss: 0.0263\n","Epoch 6: val_loss improved from 0.03110 to 0.03106, saving model to best_model_try2.h5\n","105726/105726 [==============================] - 729s 7ms/step - loss: 0.0263 - val_loss: 0.0311 - lr: 1.5000e-04\n","Epoch 7/10\n","105720/105726 [============================>.] - ETA: 0s - loss: 0.0252\n","Epoch 7: val_loss did not improve from 0.03106\n","105726/105726 [==============================] - 726s 7ms/step - loss: 0.0252 - val_loss: 0.0311 - lr: 1.5000e-04\n","Epoch 8/10\n","105726/105726 [==============================] - ETA: 0s - loss: 0.0227\n","Epoch 8: val_loss did not improve from 0.03106\n","105726/105726 [==============================] - 727s 7ms/step - loss: 0.0227 - val_loss: 0.0315 - lr: 1.5000e-05\n","Similar movies: [286897, 364, 3114, 68954, 953, 58559, 8961, 6377, 60069, 588]\n"]},{"output_type":"error","ename":"ValueError","evalue":"not enough values to unpack (expected 3, got 2)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-56-f8b09c131c21>\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_movie_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0msave_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'training_history.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"]}],"source":["# Main function\n","def main():\n","    ratings, movies = load_data()\n","    ratings, num_users, num_movies, user_id_to_index, movie_id_to_index = preprocess_data(ratings, movies)\n","\n","    movie_index_to_id = {v: k for k, v in movie_id_to_index.items()}\n","\n","    model = build_model(num_users, num_movies)\n","\n","    history = train_model(model, ratings)\n","\n","    movie_embeddings = get_movie_embeddings(model, num_movies)\n","\n","    # Example movie_ids for which to find similar movies\n","    movie_ids = [1, 2, 3]  # Example movie IDs liked by the user\n","\n","    similar_movie_ids = find_similar_movies_nn(movie_ids, movie_embeddings, movie_id_to_index, movie_index_to_id)\n","    print(f\"Similar movies: {similar_movie_ids}\")\n","\n","    return history, model, movie_embeddings\n","\n","if __name__ == '__main__':\n","    history, model, movie_embeddings = main()\n","\n","    save_history(history.history, 'training_history.pkl')\n","    plot_history(history.history)\n","\n","    # Save the movie embeddings to a file\n","    save_movie_embeddings(movie_embeddings, 'movie_embeddings.pkl')  # or 'movie_embeddings.pkl'"]},{"cell_type":"markdown","source":["## Now, let's test our data during inference time."],"metadata":{"id":"EQ8y-HU1uBmS"}},{"cell_type":"markdown","source":["Here is the the code for the main\n","\n","Let's load the model from the folder"],"metadata":{"id":"pw9v2gG_53AX"}},{"cell_type":"code","source":["model = load_model('best_model_try2.h5')"],"metadata":{"id":"brPgudgbpj3X"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Let's get the validation data\n","1. load the data\n","2. preprocess data\n","3. split data into val and train data and standardize it\n","4. Make data compatible with format of keras model\n","5. Print the standardized target values, user ids and movie ids to verify the data"],"metadata":{"id":"2XWy4C8xf0Jj"}},{"cell_type":"code","source":["# Manually split the data into training and validation sets\n","ratings, movies = load_data()\n","train_ratings, val_ratings = train_test_split(ratings, test_size=0.2, random_state=42)"],"metadata":{"id":"Asdk9_AHsaZ2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ratings, num_users, num_movies, user_id_to_index, movie_id_to_index = preprocess_data(ratings, movies)"],"metadata":{"id":"E0PqNmTRtGty"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Manually split the data into training and validation sets\n","train_ratings, val_ratings = train_test_split(ratings, test_size=0.2, random_state=42)\n","# Let's standardize the ratings\n","train_ratings, val_ratings, scaler = standardize_ratings(train_ratings, val_ratings)"],"metadata":{"id":"_mqQteYluJck"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Prepare the input data from validation set\n","user_input_val = val_ratings['user_id'].values\n","movie_input_val = val_ratings['movie_id'].values\n","targets_val = val_ratings['rating'].values"],"metadata":{"id":"eUM3n_rawFiw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# If you want you can print both arrays to see that they contain the ids for the user and movie\n","# user_input_val\n","# movie_input_val\n","# You see that the corresponding val_ratings have\n","targets_val"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BensmExdwH3V","executionInfo":{"status":"ok","timestamp":1720544586455,"user_tz":-120,"elapsed":469,"user":{"displayName":"Maximilian Summerer","userId":"08060633434699036465"}},"outputId":"c0954324-4309-4cf5-dc85-2597c1548e5c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0.33333333, 0.33333333, 0.        , ..., 0.77777778, 0.44444444,\n","       0.66666667])"]},"metadata":{},"execution_count":63}]},{"cell_type":"markdown","source":["### Predictions & Mean Absolute Error\n","Compute predictions with trained model and evaluate interpretable performance of network with unstandardized data"],"metadata":{"id":"hYH31olCgiZ-"}},{"cell_type":"code","source":["# Make predictions\n","predictions = model.predict([user_input_val, movie_input_val])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eXMO7i24wnjA","executionInfo":{"status":"ok","timestamp":1720544991886,"user_tz":-120,"elapsed":360486,"user":{"displayName":"Maximilian Summerer","userId":"08060633434699036465"}},"outputId":"f04aa2ed-59b6-48dd-aa83-f9b21f909f5c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["211452/211452 [==============================] - 282s 1ms/step\n"]}]},{"cell_type":"code","source":["predictions"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hZZMlJVSza8Y","executionInfo":{"status":"ok","timestamp":1720544997210,"user_tz":-120,"elapsed":1069,"user":{"displayName":"Maximilian Summerer","userId":"08060633434699036465"}},"outputId":"8248b4c9-c82a-4955-d683-10f86a3ec358"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.5346134 ],\n","       [0.6167196 ],\n","       [0.48491555],\n","       ...,\n","       [0.7675249 ],\n","       [0.40146667],\n","       [0.6773647 ]], dtype=float32)"]},"metadata":{},"execution_count":65}]},{"cell_type":"markdown","source":["As you can see, our algorithm predicts quite accurate. Considering you have a 5 star rating system with a stepsize of 0.5. You are on average just one step size away from the target value.\n","\n","Hence, we can use our movie_embeddings to calculate the cosine similarity."],"metadata":{"id":"tzX8qrssB38N"}},{"cell_type":"code","source":["    # Calculate Mean Absolute Error\n","    mae = mean_absolute_error(targets_val, predictions)\n","    print(f'Mean Absolute Error: {mae}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J0Jsf70Gziyk","executionInfo":{"status":"ok","timestamp":1720545029956,"user_tz":-120,"elapsed":1104,"user":{"displayName":"Maximilian Summerer","userId":"08060633434699036465"}},"outputId":"5c147061-0e49-436a-c86a-fef2446de3c9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mean Absolute Error: 0.1324289928327861\n"]}]},{"cell_type":"code","source":["# Calculate unstandardized Mean Absolute Error\n","# Unstandardize predictions\n","unstandardized_targets_val = unstandardize_ratings(targets_val, scaler)\n","unstandardized_predictions = unstandardize_ratings(predictions, scaler)\n","# Calculate Mean Absolute Error\n","mae = mean_absolute_error(unstandardized_targets_val, unstandardized_predictions)\n","print(f'Mean Absolute Error: {mae}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uDGUCr7U6PKO","executionInfo":{"status":"ok","timestamp":1720545209455,"user_tz":-120,"elapsed":1107,"user":{"displayName":"Maximilian Summerer","userId":"08060633434699036465"}},"outputId":"8537c95b-0b55-4dce-d3ff-3d02a216e19c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mean Absolute Error: 0.5959304687828529\n"]}]},{"cell_type":"markdown","source":["What I found out, is that there is no difference in performance based on standardizing the data or not. Both have the mean absolute error of 0.59"],"metadata":{"id":"bvSJm1GifLuR"}}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyMPpIof1w3vGV9OxJiui8kZ"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}